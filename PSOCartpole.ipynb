{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarm Optimisation for solving the Cartpole Problem\n",
    "\n",
    "In this implementation, we use PSO to optimise the weights of the linear function approximator that's used to create the policy for the cartpole.v1 environment.\n",
    "\n",
    "This method manages to solver the environment in 2.4 iterations (on average). This makes it highly competitive with the far more advanced implementations found on the leaderboard. However, you could make a case for saying that this algorithm requires 36 iterations (on average) to solve as we candidate solutions in the form of the swarm. Regardless, the lack of DNN and gradient calculations cause this implementation to find extremely quickly.\n",
    "\n",
    "For more details of this implementation see here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import envs\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create optimiser\n",
    "\n",
    "This is a standard version of PSO. It could be improved with parameters tunning, similar to that seen in learning rate optimisers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleSwarmOptimisation(object):\n",
    "    \n",
    "    def __init__(self, fitnessFunction, bounds, numParticles, omega, v, phiL, phiG):\n",
    "        self.bestGlobalFitness = -np.inf              \n",
    "        self.bestGlobalPos = []     \n",
    "        self.fitnessFunction = fitnessFunction\n",
    "        self.bounds = bounds \n",
    "        self.swarm = [] #Create swarm\n",
    "        for i in range(numParticles):\n",
    "            self.swarm.append(Particle(bounds, omega, v, phiL, phiG))\n",
    "\n",
    "    def maxamise(self, maxIterations, target):\n",
    "        bestGlobalFitness = -np.inf              \n",
    "        bestGlobalPos = []                \n",
    "        #Optimisation loop\n",
    "        for i in range(1, maxIterations):\n",
    "            #Evaluate each particles fitness\n",
    "            for p in self.swarm:\n",
    "                fitness = p.evaluate(self.fitnessFunction)\n",
    "                #Determine if current particle is new global best\n",
    "                if fitness > bestGlobalFitness:\n",
    "                    bestGlobalPos = p.bestPos.copy()\n",
    "                    bestGlobalFitness = p.bestFitness\n",
    "            #Update velocity and positions\n",
    "            for p in self.swarm:\n",
    "                p.updateVelocity(bestGlobalPos)\n",
    "                p.updatePosition(self.bounds)     \n",
    "            #Resample best to see if environment is solved\n",
    "            bestGlobalFitness = self.fitnessFunction(bestGlobalPos, 100)\n",
    "            print('Iteration: ' + str(i) + ' Global best: ' + str(bestGlobalFitness))\n",
    "            if bestGlobalFitness > target:\n",
    "                return i\n",
    "        return i #failed to solve\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, bounds, omega, v, phiL, phiG):\n",
    "        self.bestFitness = -np.inf     \n",
    "        self.bestPos = []\n",
    "        self.omega, self.phiL, self.phiG  = omega, phiL, phiG\n",
    "\n",
    "        self.position =  np.random.uniform(low=bounds[0], high=bounds[1], size=8)\n",
    "        self.velocity = np.random.uniform(low=-v, high=v, size=8)\n",
    "\n",
    "    # evaluate current fitness\n",
    "    def evaluate(self, fitnessFunction):\n",
    "        fitness = fitnessFunction(self.position, 25)\n",
    "        #update best position\n",
    "        if fitness > self.bestFitness:\n",
    "            self.bestPos = self.position.copy()\n",
    "            self.bestFitness = fitness\n",
    "        else:\n",
    "            #Re-evaluate best \n",
    "            self.bestFitness = fitnessFunction(self.bestPos, 25)\n",
    "        return self.bestFitness\n",
    "                    \n",
    "    # update new particle velocity\n",
    "    def updateVelocity(self, globalBest):\n",
    "        velLocal= self.phiL * np.random.rand((len(self.bestPos))) * (self.bestPos - self.position)\n",
    "        velGlobal = self.phiG * np.random.rand((len(self.bestPos))) * (globalBest - self.position)\n",
    "        self.velocity = self.omega * self.velocity + velLocal + velGlobal\n",
    "\n",
    "    # update the particle position \n",
    "    def updatePosition(self,bounds):  \n",
    "        self.position = self.position + self.velocity            \n",
    "        self.position[self.position < bounds[0]] = bounds[0]\n",
    "        self.position[self.position > bounds[1]] = bounds[1]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Fitness Function\n",
    "\n",
    "The particles position is used as the weights in a linear layer which is past through a softmax fuction to create a stochastic policy.\n",
    "\n",
    "Note the need for repeated sampling of the environment to get a good estimate of the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartpoleFitness(object):\n",
    "\n",
    "    def __init__(self, terminationStep):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.terminationStep = terminationStep\n",
    "\n",
    "    def policy(self, state, pos):\n",
    "\t    z = state.dot(pos)\n",
    "\t    exp = np.exp(z)\n",
    "\t    return exp/np.sum(exp)\n",
    "\n",
    "    def evaluate(self, pos, evaluationIterations):\n",
    "        policy = np.reshape(pos, (4,2))\n",
    "        rewardTotal = 0\n",
    "        for i in range(evaluationIterations):\n",
    "            state = self.env.reset()\n",
    "            step = 0\n",
    "            while True:\n",
    "                step += 1\n",
    "                probs = self.policy(state, policy)\n",
    "                action = np.random.choice(2,p=probs)\n",
    "                state, reward, terminal, _ = self.env.step(action)\n",
    "                rewardTotal += reward\n",
    "                if terminal or step > self.terminationStep:\n",
    "                    break\n",
    "\n",
    "        return rewardTotal  / (i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create experiment\n",
    "\n",
    "Change optimiser parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iteration: 1 Global best: 170.59\nIteration: 2 Global best: 194.35\nIteration: 3 Global best: 196.12\nTrail 1 solved in 3 iterations and 12.6 seconds.\nIteration: 1 Global best: 71.17\nIteration: 2 Global best: 83.16\nIteration: 3 Global best: 175.93\nIteration: 4 Global best: 201.0\nTrail 2 solved in 4 iterations and 10.2 seconds.\nIteration: 1 Global best: 200.64\nTrail 3 solved in 1 iterations and 1.7 seconds.\nIteration: 1 Global best: 140.37\nIteration: 2 Global best: 190.73\nIteration: 3 Global best: 201.0\nTrail 4 solved in 3 iterations and 10.0 seconds.\nIteration: 1 Global best: 179.74\nIteration: 2 Global best: 200.7\nTrail 5 solved in 2 iterations and 6.3 seconds.\nIteration: 1 Global best: 201.0\nTrail 6 solved in 1 iterations and 2.5 seconds.\nIteration: 1 Global best: 115.31\nIteration: 2 Global best: 170.3\nIteration: 3 Global best: 201.0\nTrail 7 solved in 3 iterations and 8.5 seconds.\nIteration: 1 Global best: 99.57\nIteration: 2 Global best: 201.0\nTrail 8 solved in 2 iterations and 4.3 seconds.\nIteration: 1 Global best: 154.15\nIteration: 2 Global best: 201.0\nTrail 9 solved in 2 iterations and 4.7 seconds.\nIteration: 1 Global best: 85.61\nIteration: 2 Global best: 188.24\nIteration: 3 Global best: 201.0\nTrail 10 solved in 3 iterations and 8.3 seconds.\nSolved in an average of 2.4 iterations, with an average of 6.9 seconds per trail.\n"
    }
   ],
   "source": [
    "cartpole = CartpoleFitness(200)\n",
    "\n",
    "weightSpaceBounds = 8\n",
    "numberOfParticles = 15\n",
    "momentum = 0.5\n",
    "initialVelocityBounds = 0.25\n",
    "localWeight = 2\n",
    "globalWeight = 2\n",
    "\n",
    "p = [weightSpaceBounds, numberOfParticles, momentum, initialVelocityBounds, localWeight, globalWeight]\n",
    "\n",
    "timeTotal = 0\n",
    "iterationTotal = 0\n",
    "for i in range(1, 11):\n",
    "    start = time.time()\n",
    "    solver = ParticleSwarmOptimisation(cartpole.evaluate, (-p[0],p[0]), p[1], p[2], p[3], p[4], p[5])\n",
    "    k = solver.maxamise(25, 195)\n",
    "    iterationTotal += k\n",
    "    end = time.time() - start\n",
    "    print('Trail ' + str(i) + ' solved in ' + str(k) + ' iterations and ' + \"{:.1f}\".format(end) + ' seconds.')\n",
    "    timeTotal += end\n",
    "print('Solved in an average of ' + str(iterationTotal/i) + ' iterations, with an average of ' + \"{:.1f}\".format(timeTotal /i) + ' seconds per trail.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the cartpole problem is solved in an average of 2.4 iterations, with an average of 8.3 seconds per trail. This average time could be greatly improved with the use of parallel computation and better hardware.\n",
    "\n",
    "The iteration average is very impressive, but we need to keep in mind the swarm used. In a standard RL implementation, a single agent is used, it can only run a single episode each iteration. Our swarm effectively runs 15 episodes each iteration and so, in a fair comparison between the swarm and a single agent, we would let the single-agent run for 15 times more episodes. \n",
    "\n",
    "The counter-argument to this is the single-agent requiring far more computation than a single swarm agent. In the case of a DNN implementation, there could be 1000s of weights being updated each time step. In contrast, our swarm is optimising 180 (12*15) weights between episodes. \n",
    "\n",
    "In close this out, I want get a little bit preachy.\n",
    "\n",
    "Often, in cartpole solving implementations, episodes will terminate after 500 steps (this being the limit built into the environment.) This is wrong. The solve task is considered solved after averaging >= 195 reward over 100 episodes while terminating after the 200th episode. This ensures that the solution is consistently solving the problem, which is a more difficult task than beating the 195 average when episodes can last far longer. \n",
    "\n",
    "To illustrate this point, I will rerun the previous experiment, but with a 500 episode limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iteration: 1 Global best: 89.77\nIteration: 2 Global best: 302.96\nTrail 1 solved in 2 iterations and 4.8 seconds.\nIteration: 1 Global best: 174.49\nIteration: 2 Global best: 245.61\nTrail 2 solved in 2 iterations and 5.2 seconds.\nIteration: 1 Global best: 142.46\nIteration: 2 Global best: 310.86\nTrail 3 solved in 2 iterations and 5.8 seconds.\nIteration: 1 Global best: 246.93\nTrail 4 solved in 1 iterations and 2.1 seconds.\nIteration: 1 Global best: 119.81\nIteration: 2 Global best: 403.09\nTrail 5 solved in 2 iterations and 6.2 seconds.\nIteration: 1 Global best: 89.49\nIteration: 2 Global best: 278.31\nTrail 6 solved in 2 iterations and 4.3 seconds.\nIteration: 1 Global best: 108.17\nIteration: 2 Global best: 500.0\nTrail 7 solved in 2 iterations and 7.9 seconds.\nIteration: 1 Global best: 203.35\nTrail 8 solved in 1 iterations and 2.3 seconds.\nIteration: 1 Global best: 302.57\nTrail 9 solved in 1 iterations and 3.6 seconds.\nIteration: 1 Global best: 108.23\nIteration: 2 Global best: 493.94\nTrail 10 solved in 2 iterations and 6.5 seconds.\nSolved in an average of 1.7 iterations, with an average of 4.9 seconds per trail.\n"
    }
   ],
   "source": [
    "cartpole = CartpoleFitness(500)\n",
    "\n",
    "timeTotal = 0\n",
    "iterationTotal = 0\n",
    "for i in range(1, 11):\n",
    "    start = time.time()\n",
    "    solver = ParticleSwarmOptimisation(cartpole.evaluate, (-p[0],p[0]), p[1], p[2], p[3], p[4], p[5])\n",
    "    k = solver.maxamise(25, 195)\n",
    "    iterationTotal += k\n",
    "    end = time.time() - start\n",
    "    print('Trail ' + str(i) + ' solved in ' + str(k) + ' iterations and ' + \"{:.1f}\".format(end) + ' seconds.')\n",
    "    timeTotal += end\n",
    "print('Solved in an average of ' + str(iterationTotal/i) + ' iterations, with an average of ' + \"{:.1f}\".format(timeTotal /i) + ' seconds per trail.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the average solve iterations has dropped to 1.7. Clearly, the problem has been made easier but raising the episode limit.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bit47d146cc5c834d3aadd19303fc80d1ee",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}